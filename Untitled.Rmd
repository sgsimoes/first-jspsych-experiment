---
title: "rstudio-day1"
output: html_document
date: "2023-10-17"
---

# install packages

```{r}
install.packages("tidyverse")
```

# load packages

```{r}
library(tidyverse)
```

```{r}
view(iris)
```

# plot iris

```{r}
ggplot(data = iris)+
  geom_point(mapping = aes(x = Petal.Width, y = Petal.Length, color = Species, size = Species, shape = Species))

ggplot(data = iris)+
  geom_line(mapping = aes(x = Sepal.Width, y = Sepal.Length, color = Species, size = Species, shape = Species))
```

```{r}
ggplot(data = iris)+
  geom_col(mapping = aes(x = Species, y = Petal.Length), fill = "skyblue")+theme_classic()
```

# load class data

```{r}
savic = read_csv("class_data (2).csv")

view(savic)
```

# basic info

```{r}
nrow(savic)

ncol(savic)

colnames(savic)

ggplot(data = savic)+
  geom_histogram(mapping = aes(x= as.numeric(rt)))

range(as.numeric(savic$rt))
```


# tidyverse verbs

```{R}
objectdata = read_csv("objectdata.csv") %>%
  mutate(rt = as.numeric(rt),
         weight = as.factor(weight),
         shape = as.factor(shape))

ncol(objectdata)
nrow(objectdata)

condition_data = objectdata %>%
  filter(typeoftrial == "picture" & weight %in% c("Heavy", "Light") & shape %in% c("Normal", "Smashed") & correct == TRUE) %>%
  select(subject, rt, weight, shape)


object_agg = condition_data %>%
  group_by(weight, shape) %>%
  summarize(mean_rt = mean(rt),
            sd_rt = sd(rt))

ggplot(data = object_agg) +
  geom_col(mapping = aes(x = shape, y = mean_rt, fill = weight),
           position = "dodge")+
theme_bw()+
  labs(title = "plot of RTs")+
  scale_fill_grey()
```

# load revised class data

```{r}
savic = read_csv("final_class_data.csv") %>%
  mutate(rt = as.numeric(rt),
         relatedness = as.factor(relatedness),
         type = as.factor(type))
```

# basic descriptives

```{r}
nrow(savic)
levels(savic$relatedness)

savic %>% group_by(ID) %>% count()

savic %>% filter(typeoftrial == "target") %>%
  group_by(ID) %>% count()

savic %>% filter(typeoftrial == "target") %>%
  pull(rt)

savic %>% 
  pull(ID) %>% unique()

savic %>% 
  pull(ID) %>% unique() %>% length()
```

# attention

```{r}
attention_trials = savic %>% filter(typeoftrial == "attention")

# select picks out columns
attention_trials = savic %>% filter(typeoftrial == "attention") %>% select(ID, revised_response, novel1, novel2, novel3, revised_correct)

## mean
attention_trials %>% 
  summarize(mean_accuracy = mean(revised_correct), sd_accuracy = sd(revised_correct))

## summarize participant accuracy

subject_attention_accuracy = attention_trials %>%
  group_by(ID) %>%
  summarize(mean_accuracy = mean(revised_correct))

# find IDs that have less than 75% accuracy
low_acc_IDs = subject_attention_accuracy %>%
  filter(mean_accuracy < 0.75) %>%
  pull(ID)
# 19 people have low accuracy
```

# priming trials filtering

```{r}
priming_data = savic %>% filter(typeoftrial == "target")

priming_data = savic %>% filter(typeoftrial == "target") %>%
select(ID, rt, relatedness, prime, response, type, correct, block_number, target, correct_key) %>%
filter(!is.na(rt), rt > 200, rt < 1500, correct = TRUE, block_number == 1) %>% 
  filter(relatedness %in% c("related", "unrelated") & type %in% c("direct", "shared")) %>%
  filter(!ID %in% low_acc_IDs)
```

# plot

```{r}
priming_data %>%
  group_by(type, relatedness) %>%
  summarize(mean_rt = mean(rt)) %>%
  ggplot() +
  geom_col(mapping = aes(x = type, y = mean_rt,
          group = relatedness, fill = relatedness),
           position = "dodge")+
  theme_bw()+
  scale_fill_grey()


```

# association

```{r}
scoring = read_csv("association_scoring.csv")%>%
  arrange(cue,response)

association_trials = savic %>%
  filter(typeoftrial == "association") %>%
  select(ID, revised_response, cue) %>%
  rename(response = "revised_response") %>%
  mutate(response = tolower(response)) %>%
  left_join(scoring)

congruence_trials = association_trials %>%
  filter(!is.na(congruence))%>%
  filter(congruence %in% c("congruent", "incongruent")) %>%
  filter(type_of_association %in% c("direct", "shared"))

congruence_counts = congruence_trials %>%
  group_by(ID, cue_type, congruence, type_of_association) %>%
  count() %>%
  group_by(ID, cue_type) %>%
  mutate(proportion = n / sum(n))

congruence_counts %>%
  filter(congruence == "congruent") %>%
  ungroup()%>%
  summarize(mean_prop = mean(proportion))

wide_counts = congruence_counts %>%
  select(ID, cue_type, congruence, type_of_association, proportion)%>%
  pivot_wider(names_from = congruence, values_from = proportion) %>%
  mutate(incongruent = ifelse(is.na(incongruent), 0, incongruent), congruent = ifelse(is.na(congruent), 0, congruent))%>%
  mutate(prop = congruent - incongruent)

mean(wide_counts$prop)

## counts by type of association

association_type_occurrence = wide_counts %>%
  select(ID, cue_type, type_of_association, prop) %>%
  pivot_wider(names_from = type_of_association, values_from = prop) %>%
  mutate(shared = ifelse(is.na(shared), 0, shared), direct = ifelse(is.na(direct), 0, direct))

mean(association_type_occurrence$direct)
mean(association_type_occurrence$shared)

```

# linear models

```{r}
data(women)

women %>%
  ggplot(aes(x= weight, y= height))+ 
  geom_point() +
  geom_smooth(method = "lm")+
  theme_classic()

women_model = lm(data = women, height ~ weight)
summary(women_model)

sd(women$height)
sd(women$weight)

women = women %>%
  mutate(z_height = scale(height), z_weight = scale(weight))

mean(women$z_height)
mean(women$z_weight)

sd(women$z_height)
sd(women$z_weight)

women_model_2 = lm(data = women, z_height ~ z_weight)
summary(women_model_2)

# correlation is the slope of line in our regression

women %>%
  summarize(r = cor(z_height, z_weight))

women %>%
  summarize(r = cor(height, weight))


```

# revisiting iris

```{r}
data("iris")
view(iris)

iris_subset = iris %>%
  filter(Species %in% c("setosa", "virginica"))

iris_subset %>%
  ggplot(aes(x = Species, y = Petal.Length))+
  geom_boxplot()

# comparing setosa and virginica

iris_subset_lm = lm(data = iris_subset, Petal.Length ~ Species)
summary(iris_subset_lm)

t.test(Petal.Length ~ Species, data = iris_subset)

# mean petal length of each group can be found by adding the estimates to the y-intercept

full_iris_model = lm(data = iris, Petal.Length ~ Species)
summary(full_iris_model)

full_iris_aov = aov(data = iris, Petal.Length ~ Species)
summary(full_iris_aov)

install.packages("emmeans")

emmeans::emmeans(full_iris_model, 
                 pairwise ~ Species,
                 adjust="tukey")
```

```{r}

# below are assumptions

# linearity: the relationship is actually linear in nature

# residuals: everything in your data that your model can't explain; once you take away the independent variable, the data should be normally distributed

# homoskedasticity: the variance in each group is the same

# independence of the observations: every data point is measuring something different (not the case in our study because each participant does every condition)
```

```{r}
install.packages("performance", dependencies = TRUE)
install.packages("see", dependencies = TRUE)
install.packages("patchwork", dependencies = TRUE)

library(performance)
check_model(full_iris_model)

install.packages("datarium")
data("jobsatisfaction", package = "datarium")
View(jobsatisfaction)

jobsatisfaction %>%
  ggplot()+
  geom_boxplot(aes(x = gender, y = score, color = education_level))

job_model = (lm(data = jobsatisfaction, score ~ gender + education_level + gender:education_level))
summary(job_model)

install.packages("car")

emmeans::emmeans(job_model,
                 pairwise ~ gender | education_level, adjust="tukey")


check_model(job_model)

```

```{r}
# run a multiple regression model on the priming data

rt_lm_model = lm(data = priming_data, rt ~ type + relatedness + type:relatedness)
summary(rt_lm_model)
check_model(rt_lm_model)


# run a linear mixed effects model

library(lmerTest)
rt_model = lmer(data = priming_data, rt ~ relatedness*type + (1|ID))

summary(rt_model)

car::Anova(rt_model)
```

